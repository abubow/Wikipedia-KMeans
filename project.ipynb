{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries used in this code are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting stop words from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a function to remove non-alphabatic characters from a string and a function to remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonAlphbets(word):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \"\", word)\n",
    "    re.sub(' +', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(arr):\n",
    "    return [w for w in arr if not w.lower() in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create a dictionary of words that occur in the article to base the clustering on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictionaryOfAllWords(aritcle):\n",
    "    createDictionaryOfAllWords.dictionary = {}\n",
    "    arr = aritcle.split(\" \")\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = removeNonAlphbets(arr[i])\n",
    "    unique_words = set(arr)\n",
    "    for i in unique_words:\n",
    "        if i not in createDictionaryOfAllWords.dictionary.keys():\n",
    "            createDictionaryOfAllWords.dictionary[i] = 0\n",
    "    return createDictionaryOfAllWords.dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean distance function to calculate the distance between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans clustering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(data, k):\n",
    "    centroids = data.sample(n=k)\n",
    "    clusters = [[] for _ in centroids.index]\n",
    "    if type(data) is pd.DataFrame:\n",
    "        while True:\n",
    "            old_centroids = centroids.copy()\n",
    "            for i in data.index:\n",
    "                distances = []\n",
    "                for j in centroids.index:\n",
    "                    distances.append(euclidean_distance(data.loc[i], centroids.loc[j]))\n",
    "                clusters[distances.index(np.min(distances))].append([x for x in data.loc[i]])\n",
    "            if old_centroids.equals(centroids):\n",
    "                break\n",
    "            for i in range(k):\n",
    "                centroids[i] = np.mean(clusters[i], axis=0)\n",
    "    else:\n",
    "        while True:\n",
    "            old_centroids = centroids.copy()\n",
    "            for i in data.index:\n",
    "                distances = []\n",
    "                for j in centroids.index:\n",
    "                    distances.append(euclidean_distance(data[i], centroids[j]))\n",
    "                clusters[distances.index(np.min(distances))].append(data[i])\n",
    "            if old_centroids.equals(centroids):\n",
    "                break\n",
    "            for i in range(k):\n",
    "                centroids[i] = np.mean(clusters[i], axis=0)\n",
    "\n",
    "    return centroids, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create a dictionary of words that occur in the article to base the clustering on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(article):\n",
    "    dictionary = {}\n",
    "    arr = article.strip().split(\" \")\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = removeNonAlphbets(arr[i])\n",
    "    arr = removeStopWords(arr)\n",
    "    unique_words = set(arr)\n",
    "    for i in unique_words:\n",
    "        if i not in dictionary.keys():\n",
    "            dictionary[i] = 1\n",
    "    for i in unique_words:\n",
    "        dictionary[i]=arr.count(i)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the index file containing the article titles, getting the article content from wikipedia and creating a dictionary of words that occur in the article to base the clustering on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article10 : AccessibleComputing\n",
      "\n",
      "article9 : Anarchism\n",
      "\n",
      "article8 : AfghanistanHistory\n",
      "\n",
      "article7 : AfghanistanGeography\n",
      "\n",
      "article6 : AfghanistanPeople\n",
      "\n",
      "article5 : AfghanistanCommunications\n",
      "\n",
      "article4 : AfghanistanTransportations\n",
      "\n",
      "article3 : AfghanistanMilitary\n",
      "\n",
      "article2 : AfghanistanTransnationalIssues\n",
      "\n",
      "article1 : AssistiveTechnology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AllDict = {}\n",
    "articleDictionaries = []\n",
    "count = 10 #number of articles to be used\n",
    "with open(\"D:/Wikipedia/wikipedia/enwiki-20220401-pages-articles-multistream-index.txt\") as infile:\n",
    "    \n",
    "    articles = []\n",
    "    for line in infile:\n",
    "        arr = line.split(':')\n",
    "        print(\"article{} : {}\".format(count, arr[2]))\n",
    "        article = wikipedia.page(arr[2]).content\n",
    "        articles.append([arr[2], article])\n",
    "        count -= 1\n",
    "        if count <= 0:\n",
    "            break\n",
    "    for article in articles:\n",
    "        articleDictionaries.append(frequency(article[1]))\n",
    "        dictF = createDictionaryOfAllWords(article[1])\n",
    "        for (i, j) in dictF.items():\n",
    "            AllDict[i] = j\n",
    "\n",
    "    for i in range(len(articleDictionaries)):\n",
    "        for n, v in AllDict.items():\n",
    "            if n not in articleDictionaries[i]:\n",
    "                articleDictionaries[i][n] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrting the frequency of each word in the article to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.csv\", 'w') as file:\n",
    "    (k := next(iter(AllDict)), AllDict.pop(k))\n",
    "    size = len(AllDict)\n",
    "    for i in AllDict.keys():\n",
    "        size-=1\n",
    "        if size == 0:\n",
    "            file.write(i)\n",
    "        else:\n",
    "            file.write(i + \",\")\n",
    "    file.write(\"\\n\")\n",
    "    for itr in range(len(articleDictionaries)):\n",
    "        for i, v in AllDict.items():\n",
    "            file.write(\"{},\".format(articleDictionaries[itr][i]))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>them</th>\n",
       "      <th>present</th>\n",
       "      <th>without</th>\n",
       "      <th>so</th>\n",
       "      <th>strain</th>\n",
       "      <th>to</th>\n",
       "      <th>annual</th>\n",
       "      <th>braille</th>\n",
       "      <th>key</th>\n",
       "      <th>items</th>\n",
       "      <th>...</th>\n",
       "      <th>unlit</th>\n",
       "      <th>inhibiting</th>\n",
       "      <th>offroad</th>\n",
       "      <th>accommodate</th>\n",
       "      <th>accomplish</th>\n",
       "      <th>surgical</th>\n",
       "      <th>Nursing</th>\n",
       "      <th>Patenting</th>\n",
       "      <th>healthmonitoring</th>\n",
       "      <th>sit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   them  present  without  so  strain  to  annual  braille  key  items  ...  \\\n",
       "0     1        2        0   1       0   1       1        5    1      2  ...   \n",
       "0     0       10        0   0       0   0       0        0    0      0  ...   \n",
       "0     4        1        0   0       0   0       0        2    0      0  ...   \n",
       "0     0        0        0   0       0   0       0        1    0      1  ...   \n",
       "0     0        0        0   0       0   1       0        0    0      0  ...   \n",
       "\n",
       "   unlit  inhibiting  offroad  accommodate  accomplish  surgical  Nursing  \\\n",
       "0      0           0        0            0           0         0        0   \n",
       "0      0           0        0            0           0         0        0   \n",
       "0      0           0        0            0           0         0        0   \n",
       "0      0           0        0            0           0         0        0   \n",
       "0      0           0        0            0           0         0        0   \n",
       "\n",
       "   Patenting  healthmonitoring  sit  \n",
       "0          0                 0  NaN  \n",
       "0          0                 0  NaN  \n",
       "0          0                 0  NaN  \n",
       "0          0                 0  NaN  \n",
       "0          0                 0  NaN  \n",
       "\n",
       "[5 rows x 7562 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "data = pd.read_csv(\"input.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d058ec774b5c34f18eda38579024d1056efd0b5cb515dc45fede227d42f2ef30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
